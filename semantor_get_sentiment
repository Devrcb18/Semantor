{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/devanshshukla123/semantor-get-sentiment?scriptVersionId=262198462\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"602a8e34","metadata":{"papermill":{"duration":0.002746,"end_time":"2025-09-16T15:20:24.74646","exception":false,"start_time":"2025-09-16T15:20:24.743714","status":"completed"},"tags":[]},"source":["# Semantor -"]},{"cell_type":"markdown","id":"902be038","metadata":{"papermill":{"duration":0.001818,"end_time":"2025-09-16T15:20:24.750765","exception":false,"start_time":"2025-09-16T15:20:24.748947","status":"completed"},"tags":[]},"source":["> Standard translation models often overlook the rich semantic information conveyed by punctuation, treating a question and a statement as nearly identical. The Semantor project is an experimental English-to-Hindi translation model that directly tackles this gap. Built on a PyTorch Transformer, its core innovation is a custom encoder that creates a powerful, unified embedding by fusing a word's traditional contextual meaning with a unique semantic value derived from its punctuation. By interpreting marks like '?' and '!' as direct signals of intent, the model gains a deeper, more human-like understanding of the source text, aiming to produce translations that are not just literally correct but also contextually nuanced."]},{"cell_type":"markdown","id":"029bfd5c","metadata":{"papermill":{"duration":0.001811,"end_time":"2025-09-16T15:20:24.754593","exception":false,"start_time":"2025-09-16T15:20:24.752782","status":"completed"},"tags":[]},"source":["# * Github link -[https://github.com/Devrcb18/Semantor](http://)"]},{"cell_type":"code","execution_count":1,"id":"0980f857","metadata":{"_cell_guid":"1b9bbcae-c8e5-410e-9a4e-eed300b4858b","_uuid":"1da3e0de-7279-4dd7-851d-f6abdaea26a1","execution":{"iopub.execute_input":"2025-09-16T15:20:24.760276Z","iopub.status.busy":"2025-09-16T15:20:24.7595Z","iopub.status.idle":"2025-09-16T15:20:29.280275Z","shell.execute_reply":"2025-09-16T15:20:29.279431Z"},"papermill":{"duration":4.525445,"end_time":"2025-09-16T15:20:29.282002","exception":false,"start_time":"2025-09-16T15:20:24.756557","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sacrebleu\r\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hCollecting portalocker (from sacrebleu)\r\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\r\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\r\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\r\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\r\n","Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\r\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\r\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n","Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\r\n","Installing collected packages: portalocker, sacrebleu\r\n","Successfully installed portalocker-3.2.0 sacrebleu-2.5.1\r\n"]}],"source":["!pip install sacrebleu"]},{"cell_type":"code","execution_count":2,"id":"b8fa6481","metadata":{"_cell_guid":"9247f231-5e16-4fb7-a399-0629f7951069","_uuid":"55e7e5e7-49ac-419b-82b2-fed7f493d787","collapsed":false,"execution":{"iopub.execute_input":"2025-09-16T15:20:29.28941Z","iopub.status.busy":"2025-09-16T15:20:29.288673Z","iopub.status.idle":"2025-09-16T15:20:29.29239Z","shell.execute_reply":"2025-09-16T15:20:29.291802Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.008301,"end_time":"2025-09-16T15:20:29.293429","exception":false,"start_time":"2025-09-16T15:20:29.285128","status":"completed"},"tags":[]},"outputs":[],"source":["# import pandas as pd\n","# dict=pd.read_csv('/kaggle/input/english-to-hindi-dataset/Dataset_English_Hindi.csv')\n","# vocab_dict={i:dict[0][i] } for i range(len(dict[0]))"]},{"cell_type":"code","execution_count":3,"id":"caa461a5","metadata":{"execution":{"iopub.execute_input":"2025-09-16T15:20:29.299698Z","iopub.status.busy":"2025-09-16T15:20:29.299273Z","iopub.status.idle":"2025-09-16T15:20:32.342393Z","shell.execute_reply":"2025-09-16T15:20:32.341642Z"},"papermill":{"duration":3.048428,"end_time":"2025-09-16T15:20:32.344586","exception":false,"start_time":"2025-09-16T15:20:29.296158","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\r\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.2.0)\r\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\r\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\r\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\r\n","Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\r\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\r\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n"]}],"source":["!pip install sacrebleu"]},{"cell_type":"code","execution_count":4,"id":"a8c7b0cd","metadata":{"_cell_guid":"9bacb56d-b293-4c0e-a362-2ed0d0217439","_uuid":"05c85040-e263-466f-8c95-9087c8482679","collapsed":false,"execution":{"iopub.execute_input":"2025-09-16T15:20:32.352304Z","iopub.status.busy":"2025-09-16T15:20:32.352034Z","iopub.status.idle":"2025-09-16T15:20:51.655817Z","shell.execute_reply":"2025-09-16T15:20:51.654955Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":19.309443,"end_time":"2025-09-16T15:20:51.657153","exception":false,"start_time":"2025-09-16T15:20:32.34771","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["=== English-Hindi Translation Model Training ===\n","\n","Created vocabulary with 80 tokens (IDs 0-79)\n","Vocabulary size: 80\n","Training samples: 7\n","training tokens found in vocabulary\n","\n","Training for 200 epochs on roman script...\n","------------------------------------------------------------\n","Epoch  1: Loss = 4.6326, BLEU = 0.75\n","Epoch  2: Loss = 3.9764\n","Epoch  3: Loss = 3.5491\n","Epoch  4: Loss = 3.2766\n","Epoch  5: Loss = 3.0195, BLEU = 0.00\n","Epoch  6: Loss = 2.7538\n","Epoch  7: Loss = 2.5040\n","Epoch  8: Loss = 2.2531\n","Epoch  9: Loss = 2.0343\n","Epoch 10: Loss = 1.8258, BLEU = 17.51\n","Epoch 11: Loss = 1.6321\n","Epoch 12: Loss = 1.4783\n","Epoch 13: Loss = 1.3254\n","Epoch 14: Loss = 1.2099\n","Epoch 15: Loss = 1.0918, BLEU = 30.96\n","Epoch 16: Loss = 0.9950\n","Epoch 17: Loss = 0.9157\n","Epoch 18: Loss = 0.8356\n","Epoch 19: Loss = 0.7713\n","Epoch 20: Loss = 0.7207, BLEU = 42.86\n","Epoch 21: Loss = 0.6638\n","Epoch 22: Loss = 0.6217\n","Epoch 23: Loss = 0.5841\n","Epoch 24: Loss = 0.5489\n","Epoch 25: Loss = 0.5278, BLEU = 42.86\n","Epoch 26: Loss = 0.4917\n","Epoch 27: Loss = 0.4785\n","Epoch 28: Loss = 0.4796\n","Epoch 29: Loss = 0.4531\n","Epoch 30: Loss = 0.4270, BLEU = 57.14\n","Epoch 31: Loss = 0.4186\n","Epoch 32: Loss = 0.4078\n","Epoch 33: Loss = 0.3877\n","Epoch 34: Loss = 0.3852\n","Epoch 35: Loss = 0.3786, BLEU = 42.86\n","Epoch 36: Loss = 0.3675\n","Epoch 37: Loss = 0.3683\n","Epoch 38: Loss = 0.3520\n","Epoch 39: Loss = 0.3549\n","Epoch 40: Loss = 0.3449, BLEU = 74.25\n","Epoch 41: Loss = 0.3401\n","Epoch 42: Loss = 0.3356\n","Epoch 43: Loss = 0.3372\n","Epoch 44: Loss = 0.3301\n","Epoch 45: Loss = 0.3232, BLEU = 42.86\n","Epoch 46: Loss = 0.3176\n","Epoch 47: Loss = 0.3210\n","Epoch 48: Loss = 0.3085\n","Epoch 49: Loss = 0.3236\n","Epoch 50: Loss = 0.2873, BLEU = 73.71\n","Epoch 51: Loss = 0.2989\n","Epoch 52: Loss = 0.2845\n","Epoch 53: Loss = 0.2786\n","Epoch 54: Loss = 0.2547\n","Epoch 55: Loss = 0.2465, BLEU = 74.25\n","Epoch 56: Loss = 0.2418\n","Epoch 57: Loss = 0.2270\n","Epoch 58: Loss = 0.2210\n","Epoch 59: Loss = 0.2034\n","Epoch 60: Loss = 0.1990, BLEU = 73.71\n","Epoch 61: Loss = 0.2020\n","Epoch 62: Loss = 0.1836\n","Epoch 63: Loss = 0.2021\n","Epoch 64: Loss = 0.2145\n","Epoch 65: Loss = 0.1807, BLEU = 74.25\n","Epoch 66: Loss = 0.1843\n","Epoch 67: Loss = 0.1815\n","Epoch 68: Loss = 0.1714\n","Epoch 69: Loss = 0.1689\n","Epoch 70: Loss = 0.1619, BLEU = 74.25\n","Epoch 71: Loss = 0.1595\n","Epoch 72: Loss = 0.1567\n","Epoch 73: Loss = 0.1594\n","Epoch 74: Loss = 0.1543\n","Epoch 75: Loss = 0.1634, BLEU = 74.25\n","Epoch 76: Loss = 0.1561\n","Epoch 77: Loss = 0.1556\n","Epoch 78: Loss = 0.1417\n","Epoch 79: Loss = 0.1437\n","Epoch 80: Loss = 0.1414, BLEU = 74.25\n","Epoch 81: Loss = 0.1377\n","Epoch 82: Loss = 0.1456\n","Epoch 83: Loss = 0.1364\n","Epoch 84: Loss = 0.1363\n","Epoch 85: Loss = 0.1388, BLEU = 73.71\n","Epoch 86: Loss = 0.1334\n","Epoch 87: Loss = 0.1306\n","Epoch 88: Loss = 0.1392\n","Epoch 89: Loss = 0.1373\n","Epoch 90: Loss = 0.1323, BLEU = 74.25\n","Epoch 91: Loss = 0.1358\n","Epoch 92: Loss = 0.1353\n","Epoch 93: Loss = 0.1354\n","Epoch 94: Loss = 0.1369\n","Epoch 95: Loss = 0.1353, BLEU = 74.25\n","Epoch 96: Loss = 0.1241\n","Epoch 97: Loss = 0.1281\n","Epoch 98: Loss = 0.1296\n","Epoch 99: Loss = 0.1282\n","Epoch 100: Loss = 0.1336, BLEU = 74.25\n","Epoch 101: Loss = 0.1250\n","Epoch 102: Loss = 0.1271\n","Epoch 103: Loss = 0.1246\n","Epoch 104: Loss = 0.1328\n","Epoch 105: Loss = 0.1306, BLEU = 74.25\n","Epoch 106: Loss = 0.1242\n","Epoch 107: Loss = 0.1204\n","Epoch 108: Loss = 0.1206\n","Epoch 109: Loss = 0.1246\n","Epoch 110: Loss = 0.1234, BLEU = 73.71\n","Epoch 111: Loss = 0.1294\n","Epoch 112: Loss = 0.1203\n","Epoch 113: Loss = 0.1259\n","Epoch 114: Loss = 0.1298\n","Epoch 115: Loss = 0.1194, BLEU = 74.25\n","Epoch 116: Loss = 0.1248\n","Epoch 117: Loss = 0.1224\n","Epoch 118: Loss = 0.1201\n","Epoch 119: Loss = 0.1204\n","Epoch 120: Loss = 0.1169, BLEU = 74.25\n","Epoch 121: Loss = 0.1152\n","Epoch 122: Loss = 0.1164\n","Epoch 123: Loss = 0.1162\n","Epoch 124: Loss = 0.1211\n","Epoch 125: Loss = 0.1217, BLEU = 74.25\n","Epoch 126: Loss = 0.1150\n","Epoch 127: Loss = 0.1137\n","Epoch 128: Loss = 0.1149\n","Epoch 129: Loss = 0.1162\n","Epoch 130: Loss = 0.1141, BLEU = 74.25\n","Epoch 131: Loss = 0.1129\n","Epoch 132: Loss = 0.1147\n","Epoch 133: Loss = 0.1161\n","Epoch 134: Loss = 0.1160\n","Epoch 135: Loss = 0.1197, BLEU = 73.71\n","Epoch 136: Loss = 0.1166\n","Epoch 137: Loss = 0.1128\n","Epoch 138: Loss = 0.1183\n","Epoch 139: Loss = 0.1180\n","Epoch 140: Loss = 0.1170, BLEU = 74.25\n","Epoch 141: Loss = 0.1114\n","Epoch 142: Loss = 0.1131\n","Epoch 143: Loss = 0.1143\n","Epoch 144: Loss = 0.1121\n","Epoch 145: Loss = 0.1130, BLEU = 74.25\n","Epoch 146: Loss = 0.1099\n","Epoch 147: Loss = 0.1120\n","Epoch 148: Loss = 0.1112\n","Epoch 149: Loss = 0.1145\n","Epoch 150: Loss = 0.1109, BLEU = 74.25\n","Epoch 151: Loss = 0.1152\n","Epoch 152: Loss = 0.1170\n","Epoch 153: Loss = 0.1111\n","Epoch 154: Loss = 0.1135\n","Epoch 155: Loss = 0.1089, BLEU = 74.25\n","Epoch 156: Loss = 0.1109\n","Epoch 157: Loss = 0.1116\n","Epoch 158: Loss = 0.1136\n","Epoch 159: Loss = 0.1116\n","Epoch 160: Loss = 0.1136, BLEU = 74.25\n","Epoch 161: Loss = 0.1087\n","Epoch 162: Loss = 0.1092\n","Epoch 163: Loss = 0.1097\n","Epoch 164: Loss = 0.1101\n","Epoch 165: Loss = 0.1077, BLEU = 74.25\n","Epoch 166: Loss = 0.1139\n","Epoch 167: Loss = 0.1133\n","Epoch 168: Loss = 0.1103\n","Epoch 169: Loss = 0.1123\n","Epoch 170: Loss = 0.1108, BLEU = 74.25\n","Epoch 171: Loss = 0.1102\n","Epoch 172: Loss = 0.1093\n","Epoch 173: Loss = 0.1077\n","Epoch 174: Loss = 0.1109\n","Epoch 175: Loss = 0.1111, BLEU = 74.25\n","Epoch 176: Loss = 0.1115\n","Epoch 177: Loss = 0.1110\n","Epoch 178: Loss = 0.1088\n","Epoch 179: Loss = 0.1075\n","Epoch 180: Loss = 0.1081, BLEU = 74.25\n","Epoch 181: Loss = 0.1088\n","Epoch 182: Loss = 0.1082\n","Epoch 183: Loss = 0.1101\n","Epoch 184: Loss = 0.1106\n","Epoch 185: Loss = 0.1069, BLEU = 74.25\n","Epoch 186: Loss = 0.1091\n","Epoch 187: Loss = 0.1092\n","Epoch 188: Loss = 0.1089\n","Epoch 189: Loss = 0.1102\n","Epoch 190: Loss = 0.1066, BLEU = 74.25\n","Epoch 191: Loss = 0.1096\n","Epoch 192: Loss = 0.1111\n","Epoch 193: Loss = 0.1060\n","Epoch 194: Loss = 0.1066\n","Epoch 195: Loss = 0.1073, BLEU = 74.25\n","Epoch 196: Loss = 0.1085\n","Epoch 197: Loss = 0.1088\n","Epoch 198: Loss = 0.1058\n","Epoch 199: Loss = 0.1067\n","Epoch 200: Loss = 0.1066, BLEU = 74.25\n","\n","------------------------------------------------------------------------------------------------------------------------\n","\n","=== Translation Examples ===\n","english-- Hello, how are you?\n","hindi-- namaste aap kaise hain\n","----------------------------------------\n","english-- I want to eat rice and dal.\n","hindi-- main chawal aur daal khaana chahta hun\n","----------------------------------------\n","english-- Good morning.\n","hindi-- suprabhat\n","----------------------------------------\n","english-- What is your name?\n","hindi-- pustak kahan hai\n","----------------------------------------\n","english-- Thank you very much.\n","hindi-- bahut dhanyawad\n","----------------------------------------\n","\n","=== Performance Metrics ===\n","Corpus BLEU Score: 72.82\n","Average Sentence BLEU: 74.25\n","Average BLEU-1: 74.25\n","Average BLEU-2: 74.25\n","Average BLEU-4: 74.25\n","\n","Loss reduction: 4.6326 → 0.1066\n","BLEU improvement: 0.75 → 74.25\n","\n","=== Model Trained ===\n","You can now use trained_model.generate_translation() to translate new sentences!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from typing import List, Dict, Tuple\n","import sacrebleu\n","from collections import Counter\n","import random\n","\n","class SentenceEmbedderWithAttention(nn.Module):\n","    def __init__(self, sem_dim: int = 8, model_dim: int = 8, num_heads: int = 2):\n","        super().__init__()\n","        self.sem_dim = sem_dim\n","        self.model_dim = model_dim\n","        self.embed_dim = sem_dim + model_dim\n","        self.num_heads = num_heads\n","        self.head_dim = self.embed_dim // num_heads\n","        \n","        assert self.embed_dim % num_heads == 0, f\"embed_dim ({self.embed_dim}) must be divisible by num_heads ({num_heads})\"\n","        \n","        self.punctuation = {\n","            ',': 'continuation',\n","            '.': 'declaration', \n","            '?': 'question',\n","            '!': 'intense_emotion',\n","            '\"': 'quotes',\n","            '(': 'detail_start',\n","            ')': 'detail_end',\n","            ';': 'pause',\n","            ':': 'explanation',\n","            '-': 'dash'\n","        }\n","        self.punct2indx = {p: i+1 for i, p in enumerate(self.punctuation)}\n","        self.punct2indx['None'] = 0\n","        self.num_punct = len(self.punct2indx)\n","        \n","        self.sem_emb = self._init_semantic_embedding()\n","        self.W_q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n","        self.W_k = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n","        self.W_v = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n","        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n","        self.layer_norm = nn.LayerNorm(self.embed_dim)\n","        self.ffn = nn.Sequential(\n","                nn.Linear(self.embed_dim, self.embed_dim * 4),\n","                nn.ReLU(),\n","                nn.Linear(self.embed_dim * 4, self.embed_dim)\n","               )\n","\n","    def _init_semantic_embedding(self) -> nn.Embedding:\n","        embedding = nn.Embedding(self.num_punct, self.sem_dim)\n","        nn.init.uniform_(embedding.weight, -0.1, 0.1)\n","        return embedding\n","\n","    def _get_positional_encoding(self, position: int, d_model: int) -> np.ndarray:\n","        pos_enc = np.zeros(d_model)\n","        for i in range(0, d_model, 2):\n","            pos_enc[i] = np.sin(position / (10000 ** (i / d_model)))\n","            if i + 1 < d_model:\n","                pos_enc[i+1] = np.cos(position / (10000 ** (i / d_model)))\n","        return pos_enc\n","\n","    def _get_semantic_embeddings(self, sentence: str) -> np.ndarray:\n","        semantic_data = []\n","        words = sentence.split()\n","\n","        for word in words:\n","            punct_idx = 0\n","            for punct in self.punctuation.keys():\n","                if word.endswith(punct):\n","                    punct_idx = self.punct2indx[punct]\n","                    break\n","            emb_vec = self.sem_emb(torch.tensor([punct_idx], dtype=torch.long))\n","            semantic_data.append(emb_vec.detach().numpy()[0])\n","\n","        return np.array(semantic_data)\n","\n","    def embed_sentence(self, sentence: str) -> torch.Tensor:\n","        words = sentence.split()\n","        if len(words) == 0:\n","            return torch.empty(0, self.embed_dim)\n","\n","        semantic_embeddings = torch.tensor(\n","            self._get_semantic_embeddings(sentence), dtype=torch.float32\n","        )\n","        # Fix the numpy array creation warning\n","        pos_encodings = np.array([self._get_positional_encoding(idx, self.model_dim) for idx in range(len(words))])\n","        positional_encodings = torch.tensor(pos_encodings, dtype=torch.float32)\n","        combined_embeddings = torch.cat((semantic_embeddings, positional_encodings), dim=1)\n","        return combined_embeddings \n","\n","    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n","        batch_size, seq_len, _ = x.size()\n","        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","    def self_attention(self, embeddings: torch.Tensor) -> torch.Tensor:\n","        x = embeddings.unsqueeze(0)  \n","        Q = self.W_q(x)\n","        K = self.W_k(x)\n","        V = self.W_v(x)\n","        Q, K, V = map(self._split_heads, (Q, K, V)) \n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n","        attn_weights = torch.softmax(scores, dim=-1)\n","        out = torch.matmul(attn_weights, V) \n","        out = out.transpose(1, 2).contiguous().view(1, -1, self.embed_dim) \n","        out = self.out_proj(out)\n","        return out.squeeze(0) \n","\n","    def forward_with_attention(self, sentence: str) -> torch.Tensor:\n","        embeddings = self.embed_sentence(sentence) \n","        attended = self.self_attention(embeddings)\n","        x = self.layer_norm(embeddings + attended)\n","        ffn_out = self.ffn(x)\n","        output = self.layer_norm(x + ffn_out)\n","        return output\n","\n","\n","class SimpleDecoder(nn.Module):\n","    def __init__(self, embed_dim, vocab_size, num_heads=2):\n","        super().__init__()\n","        self.embed_dim = embed_dim\n","        self.vocab_size = vocab_size\n","        \n","        \n","        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n","        \n","        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n","        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(embed_dim, embed_dim * 4),\n","            nn.ReLU(),\n","            nn.Linear(embed_dim * 4, embed_dim)\n","        )\n","        self.out_proj = nn.Linear(embed_dim, vocab_size)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        self.norm3 = nn.LayerNorm(embed_dim)\n","    \n","    def forward(self, tgt_tokens, memory):\n","        if isinstance(tgt_tokens, torch.Tensor) and tgt_tokens.dtype == torch.long:\n","            tgt_embeddings = self.token_embedding(tgt_tokens)\n","        else:\n","            tgt_embeddings = tgt_tokens\n","            \n","        if tgt_embeddings.dim() == 2:\n","            tgt_embeddings = tgt_embeddings.unsqueeze(0)\n","        if memory.dim() == 2:\n","            memory = memory.unsqueeze(0)\n","        \n","        seq_len = tgt_embeddings.size(1)\n","        causal_mask = self._causal_mask(seq_len)\n","        \n","        attn_out, _ = self.self_attn(tgt_embeddings, tgt_embeddings, tgt_embeddings, \n","                                    attn_mask=causal_mask, is_causal=True)\n","        x = self.norm1(tgt_embeddings + attn_out)\n","        attn_out, _ = self.cross_attn(x, memory, memory)\n","        x = self.norm2(x + attn_out)\n","        \n","        \n","        ffn_out = self.ffn(x)\n","        x = self.norm3(x + ffn_out)\n","        \n","        \n","        logits = self.out_proj(x)\n","        return logits.squeeze(0) if logits.size(0) == 1 else logits\n","    \n","    def _causal_mask(self, size):\n","        mask = torch.triu(torch.ones(size, size), diagonal=1)\n","        return mask.masked_fill(mask == 1, float('-inf'))\n","\n","\n","class TranslationTrainer:\n","    def __init__(self, encoder, decoder, vocab_dict, learning_rate=0.001):\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.vocab_dict = vocab_dict\n","        self.reverse_vocab = {v: k for k, v in vocab_dict.items()}\n","        \n","        self.criterion = nn.CrossEntropyLoss(ignore_index=0) \n","        \n","        self.optimizer = optim.Adam(\n","            list(encoder.parameters()) + list(decoder.parameters()), \n","            lr=learning_rate,\n","            weight_decay=1e-4\n","        )\n","        \n","        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)\n","        \n","        self.loss_history = []\n","        self.bleu_history = []\n","    \n","    def tokens_to_ids(self, tokens: List[str]) -> List[int]:\n","        \n","        ids = []\n","        for token in tokens:\n","            if token in self.reverse_vocab:\n","                ids.append(self.reverse_vocab[token])\n","            else:\n","                print(f\"Warning: Unknown token '{token}' replaced with <UNK>\")\n","                ids.append(self.reverse_vocab['<UNK>'])\n","        return ids\n","    \n","    def prepare_target_sequence(self, target_tokens: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n","\n","        target_with_bos = ['<BOS>'] + target_tokens\n","        target_with_eos = target_tokens + ['<EOS>']\n","        \n","        input_ids = torch.tensor(self.tokens_to_ids(target_with_bos), dtype=torch.long)\n","        target_ids = torch.tensor(self.tokens_to_ids(target_with_eos), dtype=torch.long)\n","        \n","        return input_ids, target_ids\n","    \n","    def train_step(self, source_sentence: str, target_tokens: List[str]) -> float:\n","        self.encoder.train()\n","        self.decoder.train()\n","        self.optimizer.zero_grad()\n","        \n","        encoder_outputs = self.encoder.forward_with_attention(source_sentence)\n","        \n","    \n","        decoder_input, target_labels = self.prepare_target_sequence(target_tokens)\n","    \n","        logits = self.decoder(decoder_input, encoder_outputs)\n","        \n","    \n","        loss = self.criterion(logits.view(-1, logits.size(-1)), target_labels.view(-1))\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(\n","            list(self.encoder.parameters()) + list(self.decoder.parameters()), \n","            max_norm=1.0\n","        )\n","        \n","        self.optimizer.step()\n","        \n","        return loss.item()\n","    \n","    def train_epoch(self, train_data: List[Dict], script_type='roman') -> float:\n","        total_loss = 0.0\n","        num_batches = 0\n","    \n","        shuffled_data = train_data.copy()\n","        random.shuffle(shuffled_data)\n","        \n","        for sample in shuffled_data:\n","            source = sample['source']\n","            target_key = f'target_{script_type}'\n","            \n","            if target_key in sample:\n","                target = sample[target_key]\n","                loss = self.train_step(source, target)\n","                total_loss += loss\n","                num_batches += 1\n","        \n","        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n","        self.loss_history.append(avg_loss)\n","        \n","        self.scheduler.step()\n","        \n","        return avg_loss\n","    \n","    def generate_translation(self, source_sentence: str, max_length: int = 15) -> List[str]:\n","        \"\"\"Generate translation using greedy decoding\"\"\"\n","        self.encoder.eval()\n","        self.decoder.eval()\n","        \n","        with torch.no_grad():\n","            encoder_outputs = self.encoder.forward_with_attention(source_sentence)\n","            current_tokens = [self.reverse_vocab['<BOS>']]\n","            generated_tokens = []\n","            \n","            for _ in range(max_length):\n","                input_tensor = torch.tensor(current_tokens, dtype=torch.long)\n","                logits = self.decoder(input_tensor, encoder_outputs)\n","                next_token_id = torch.argmax(logits[-1]).item()\n","                next_token = self.vocab_dict[next_token_id]\n","                if next_token == '<EOS>':\n","                    break\n","                if next_token not in {'<BOS>', '<PAD>', '<UNK>'}:\n","                    generated_tokens.append(next_token)\n","                current_tokens.append(next_token_id)\n","            \n","            return generated_tokens\n","\n","\n","class EnglishHindiTranslationEvaluator:\n","    def __init__(self, vocab_dict: Dict[int, str]):\n","        self.vocab_dict = vocab_dict\n","        self.reverse_vocab = {v: k for k, v in vocab_dict.items()}\n","        self.special_tokens = {'<BOS>', '<EOS>', '<PAD>', '<UNK>'}\n","    \n","    def calculate_bleu_metrics(self, predicted_tokens: List[str], \n","                              reference_tokens: List[str]) -> Dict[str, float]:\n","        if not predicted_tokens or not reference_tokens:\n","            return {'bleu': 0.0, 'bleu_1': 0.0, 'bleu_2': 0.0, 'bleu_4': 0.0}\n","        \n","        predicted_sentence = ' '.join(predicted_tokens)\n","        reference_sentence = ' '.join(reference_tokens)\n","        \n","        bleu = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence])\n","\n","        try:\n","            bleu_1 = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence], \n","                                           max_ngram_order=1)\n","            bleu_2 = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence], \n","                                           max_ngram_order=2)\n","            bleu_4 = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence], \n","                                           max_ngram_order=4)\n","        except:\n","            bleu_1 = bleu_2 = bleu_4 = bleu\n","        \n","        return {\n","            'bleu': bleu.score,\n","            'bleu_1': bleu_1.score,\n","            'bleu_2': bleu_2.score,\n","            'bleu_4': bleu_4.score,\n","            'brevity_penalty': bleu.bp,\n","            'length_ratio': len(predicted_tokens) / len(reference_tokens) if reference_tokens else 0\n","        }\n","    \n","    def evaluate_model(self, trainer, test_data: List[Dict], script_type='roman') -> Dict:\n","        \"\"\"Evaluate model on test data\"\"\"\n","        predictions = []\n","        references = []\n","        individual_scores = []\n","        \n","        target_key = f'target_{script_type}'\n","        \n","        for sample in test_data:\n","            source = sample['source']\n","            reference = sample[target_key]\n","            prediction = trainer.generate_translation(source)\n","            bleu_scores = self.calculate_bleu_metrics(prediction, reference)\n","            individual_scores.append(bleu_scores)\n","            \n","            predictions.append(prediction)\n","            references.append(reference)\n","        \n","        # Calculate average sentence BLEU\n","        avg_bleu = np.mean([s['bleu'] for s in individual_scores])\n","        \n","        pred_sentences = [' '.join(tokens) for tokens in predictions]\n","        ref_sentences = [' '.join(tokens) for tokens in references]\n","        \n","        if pred_sentences and ref_sentences:\n","            corpus_bleu = sacrebleu.corpus_bleu(pred_sentences, [ref_sentences])\n","        else:\n","            corpus_bleu = sacrebleu.BLEU.compute_bleu([],[])\n","        \n","        avg_bleu_1 = np.mean([s['bleu_1'] for s in individual_scores])\n","        avg_bleu_2 = np.mean([s['bleu_2'] for s in individual_scores])\n","        avg_bleu_4 = np.mean([s['bleu_4'] for s in individual_scores])\n","        \n","        return {\n","            'corpus_bleu': corpus_bleu.score,\n","            'avg_sentence_bleu': avg_bleu,  # Fixed: this variable is now properly defined\n","            'avg_bleu_1': avg_bleu_1,\n","            'avg_bleu_2': avg_bleu_2,\n","            'avg_bleu_4': avg_bleu_4,\n","            'individual_scores': individual_scores,\n","            'predictions': predictions,\n","            'references': references\n","        }\n","\n","\n","def create_english_hindi_vocab():\n","    vocab_list = [\n","    \n","        '<PAD>', '<BOS>', '<EOS>', '<UNK>',\n","        'i', 'want', 'to', 'eat', 'rice', 'and', 'dal', 'hello', 'how', 'are', 'you', \n","        'good', 'morning', 'what', 'is', 'your', 'name', 'thank', 'very', 'much', \n","        'the', 'book', 'water', 'please', 'where', 'go',\n","    \n","        'मैं', 'चावल', 'और', 'दाल', 'खाना', 'चाहता', 'हूं', 'नमस्ते', 'आप', 'कैसे',\n","        'हैं', 'सुप्रभात', 'आपका', 'नाम', 'क्या', 'है', 'बहुत', 'धन्यवाद', 'पुस्तक', \n","        'पानी', 'कृपया', 'कहां', 'जाना',\n","    \n","        'main', 'chawal', 'aur', 'daal', 'khaana', 'chahta', 'hun', 'namaste', 'aap', \n","        'kaise', 'hain', 'suprabhat', 'aapka', 'naam', 'kya', 'hai', 'bahut', \n","        'dhanyawad', 'pustak', 'paani', 'kripaya', 'kahan', 'jaana',\n","        '.', ',', '?', '!'\n","    ]\n","    \n","\n","    vocab_dict = {i: token for i, token in enumerate(vocab_list)}\n","    \n","    print(f\"Created vocabulary with {len(vocab_dict)} tokens (IDs 0-{len(vocab_dict)-1})\")\n","    return vocab_dict\n","\n","\n","def create_english_hindi_training_data():\n","    training_data = [\n","        {\n","            'source': 'I want to eat rice and dal.',\n","            'target_devanagari': ['मैं', 'चावल', 'और', 'दाल', 'खाना', 'चाहता', 'हूं'],\n","            'target_roman': ['main', 'chawal', 'aur', 'daal', 'khaana', 'chahta', 'hun']\n","        },\n","        {\n","            'source': 'Hello, how are you?',\n","            'target_devanagari': ['नमस्ते', 'आप', 'कैसे', 'हैं'],\n","            'target_roman': ['namaste', 'aap', 'kaise', 'hain']\n","        },\n","        {\n","            'source': 'Good morning.',\n","            'target_devanagari': ['सुप्रभात'],\n","            'target_roman': ['suprabhat']\n","        },\n","        {\n","            'source': 'What is your name?',\n","            'target_devanagari': ['आपका', 'नाम', 'क्या', 'है'],\n","            'target_roman': ['aapka', 'naam', 'kya', 'hai']\n","        },\n","        {\n","            'source': 'Thank you very much.',\n","            'target_devanagari': ['बहुत', 'धन्यवाद'],\n","            'target_roman': ['bahut', 'dhanyawad']\n","        },\n","        {\n","            'source': 'I want water please.',\n","            'target_devanagari': ['मैं', 'पानी', 'चाहता', 'हूं', 'कृपया'],\n","            'target_roman': ['main', 'paani', 'chahta', 'hun', 'kripaya']\n","        },\n","        {\n","            'source': 'Where is the book?',\n","            'target_devanagari': ['पुस्तक', 'कहां', 'है'],\n","            'target_roman': ['pustak', 'kahan', 'hai']\n","        },\n","    ]\n","    return training_data\n","\n","\n","def validate_training_data_vocab(training_data, vocab_dict):\n","    \"\"\"Validate that all tokens in training data exist in vocabulary\"\"\"\n","    reverse_vocab = {v: k for k, v in vocab_dict.items()}\n","    missing_tokens = set()\n","    \n","    for sample in training_data:\n","        for target_type in ['target_roman', 'target_devanagari']:\n","            if target_type in sample:\n","                for token in sample[target_type]:\n","                    if token not in reverse_vocab:\n","                        missing_tokens.add(token)\n","    \n","    if missing_tokens:\n","        print(\"Missing tokens in vocabulary:\")\n","        for token in sorted(missing_tokens):\n","            print(f\"  '{token}'\")\n","        return False\n","    return True\n","\n","\n","def train_and_evaluate_model():\n","    print(\"=== English-Hindi Translation Model Training ===\\n\")\n","    vocab_dict = create_english_hindi_vocab()\n","    training_data = create_english_hindi_training_data()\n","    \n","    print(f\"Vocabulary size: {len(vocab_dict)}\")\n","    print(f\"Training samples: {len(training_data)}\")\n","\n","    if not validate_training_data_vocab(training_data, vocab_dict):\n","        print(\"ERROR\")\n","        return None, None, None\n","    else:\n","        print(\"training tokens found in vocabulary\")\n","    \n","    \n","    encoder = SentenceEmbedderWithAttention(sem_dim=12, model_dim=12, num_heads=3)\n","    decoder = SimpleDecoder(embed_dim=24, vocab_size=len(vocab_dict), num_heads=3)\n","    \n","    trainer = TranslationTrainer(encoder, decoder, vocab_dict, learning_rate=0.002)\n","    evaluator = EnglishHindiTranslationEvaluator(vocab_dict)\n","\n","    num_epochs = 200\n","    script_type = 'roman' \n","    \n","    print(f\"\\nTraining for {num_epochs} epochs on {script_type} script...\")\n","    print(\"-\" * 60)\n","    \n","    try:\n","        for epoch in range(num_epochs):\n","            avg_loss = trainer.train_epoch(training_data, script_type)\n","\n","            if (epoch + 1) % 5 == 0 or epoch == 0:\n","                results = evaluator.evaluate_model(trainer, training_data, script_type)\n","                trainer.bleu_history.append(results['avg_sentence_bleu'])\n","                \n","                print(f\"Epoch {epoch+1:2d}: Loss = {avg_loss:.4f}, BLEU = {results['avg_sentence_bleu']:.2f}\")\n","            else:\n","                print(f\"Epoch {epoch+1:2d}: Loss = {avg_loss:.4f}\")\n","                \n","    except Exception as e:\n","        print(f\"Training failed with error: {e}\")\n","        return None, None, None\n","    \n","    print(\"\\n\" + \"--\"*60)\n","    \n","    print(\"\\n=== Translation Examples ===\")\n","    test_sentences = [\n","        \"Hello, how are you?\",\n","        \"I want to eat rice and dal.\",\n","        \"Good morning.\",\n","        \"What is your name?\",\n","        \"Thank you very much.\"\n","    ]\n","    \n","    for sentence in test_sentences:\n","        try:\n","            prediction = trainer.generate_translation(sentence)\n","            print(f\"english-- {sentence}\")\n","            print(f\"hindi-- {' '.join(prediction)}\")\n","            print(\"-\" * 40)\n","        except Exception as e:\n","            print(f\"Translation failed for '{sentence}': {e}\")\n","    \n","    try:\n","        final_results = evaluator.evaluate_model(trainer, training_data, script_type)\n","        \n","        print(\"\\n=== Performance Metrics ===\")\n","        print(f\"Corpus BLEU Score: {final_results['corpus_bleu']:.2f}\")\n","        print(f\"Average Sentence BLEU: {final_results['avg_sentence_bleu']:.2f}\")\n","        print(f\"Average BLEU-1: {final_results['avg_bleu_1']:.2f}\")\n","        print(f\"Average BLEU-2: {final_results['avg_bleu_2']:.2f}\")\n","        print(f\"Average BLEU-4: {final_results['avg_bleu_4']:.2f}\")\n","        \n","        print(f\"\\nLoss reduction: {trainer.loss_history[0]:.4f} → {trainer.loss_history[-1]:.4f}\")\n","        if len(trainer.bleu_history) > 1:\n","            print(f\"BLEU improvement: {trainer.bleu_history[0]:.2f} → {trainer.bleu_history[-1]:.2f}\")\n","        \n","        return trainer, evaluator, final_results\n","        \n","    except Exception as e:\n","        print(f\"Final evaluation failed: {e}\")\n","        return trainer, evaluator, None\n","\n","if __name__ == \"__main__\":\n","    torch.manual_seed(42)\n","    np.random.seed(42)\n","    random.seed(42)\n","    \n","    trained_model, evaluator, results = train_and_evaluate_model()\n","    \n","    print(\"\\n=== Model Trained ===\")\n","    print(\"You can now use trained_model.generate_translation() to translate new sentences!\")"]},{"cell_type":"code","execution_count":5,"id":"e5787bf6","metadata":{"execution":{"iopub.execute_input":"2025-09-16T15:20:51.669583Z","iopub.status.busy":"2025-09-16T15:20:51.669261Z","iopub.status.idle":"2025-09-16T15:20:51.681203Z","shell.execute_reply":"2025-09-16T15:20:51.680627Z"},"papermill":{"duration":0.019117,"end_time":"2025-09-16T15:20:51.682335","exception":false,"start_time":"2025-09-16T15:20:51.663218","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['namaste', 'aap', 'kaise', 'hain']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["trained_model.generate_translation('Hello, how are you?')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":8187854,"sourceId":12938951,"sourceType":"datasetVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":33.236492,"end_time":"2025-09-16T15:20:53.908558","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-16T15:20:20.672066","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}