{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12938951,"sourceType":"datasetVersion","datasetId":8187854}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"_uuid":"1da3e0de-7279-4dd7-851d-f6abdaea26a1","_cell_guid":"1b9bbcae-c8e5-410e-9a4e-eed300b4858b","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:23:11.673640Z","iopub.execute_input":"2025-09-15T05:23:11.673873Z","iopub.status.idle":"2025-09-15T05:23:16.505926Z","shell.execute_reply.started":"2025-09-15T05:23:11.673849Z","shell.execute_reply":"2025-09-15T05:23:16.505253Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Semantor-","metadata":{}},{"cell_type":"markdown","source":"Standard translation models often overlook the rich semantic information conveyed by punctuation, treating a question and a statement as nearly identical. The Semantor project is an experimental English-to-Hindi translation model that directly tackles this gap. Built on a PyTorch Transformer, its core innovation is a custom encoder that creates a powerful, unified embedding by fusing a word's traditional contextual meaning with a unique semantic value derived from its punctuation. By interpreting marks like '?' and '!' as direct signals of intent, the model gains a deeper, more human-like understanding of the source text, aiming to produce translations that are not just literally correct but also contextually nuanced.","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# dict=pd.read_csv('/kaggle/input/english-to-hindi-dataset/Dataset_English_Hindi.csv')\n# vocab_dict={i:dict[0][i] } for i range(len(dict[0]))","metadata":{"_uuid":"55e7e5e7-49ac-419b-82b2-fed7f493d787","_cell_guid":"9247f231-5e16-4fb7-a399-0629f7951069","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-15T05:23:16.507631Z","iopub.execute_input":"2025-09-15T05:23:16.507865Z","iopub.status.idle":"2025-09-15T05:23:16.511300Z","shell.execute_reply.started":"2025-09-15T05:23:16.507842Z","shell.execute_reply":"2025-09-15T05:23:16.510678Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:23:16.512077Z","iopub.execute_input":"2025-09-15T05:23:16.512300Z","iopub.status.idle":"2025-09-15T05:23:19.504585Z","shell.execute_reply.started":"2025-09-15T05:23:16.512280Z","shell.execute_reply":"2025-09-15T05:23:19.503670Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.2.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport sacrebleu\nfrom collections import Counter\nimport random\n\nclass SentenceEmbedderWithAttention(nn.Module):\n    def __init__(self, sem_dim: int = 8, model_dim: int = 8, num_heads: int = 2):\n        super().__init__()\n        self.sem_dim = sem_dim\n        self.model_dim = model_dim\n        self.embed_dim = sem_dim + model_dim\n        self.num_heads = num_heads\n        self.head_dim = self.embed_dim // num_heads\n        \n        assert self.embed_dim % num_heads == 0, f\"embed_dim ({self.embed_dim}) must be divisible by num_heads ({num_heads})\"\n        \n        self.punctuation = {\n            ',': 'continuation',\n            '.': 'declaration', \n            '?': 'question',\n            '!': 'intense_emotion',\n            '\"': 'quotes',\n            '(': 'detail_start',\n            ')': 'detail_end',\n            ';': 'pause',\n            ':': 'explanation',\n            '-': 'dash'\n        }\n        self.punct2indx = {p: i+1 for i, p in enumerate(self.punctuation)}\n        self.punct2indx['None'] = 0\n        self.num_punct = len(self.punct2indx)\n        \n        self.sem_emb = self._init_semantic_embedding()\n        self.W_q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.W_k = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.W_v = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.layer_norm = nn.LayerNorm(self.embed_dim)\n        self.ffn = nn.Sequential(\n                nn.Linear(self.embed_dim, self.embed_dim * 4),\n                nn.ReLU(),\n                nn.Linear(self.embed_dim * 4, self.embed_dim)\n               )\n\n    def _init_semantic_embedding(self) -> nn.Embedding:\n        embedding = nn.Embedding(self.num_punct, self.sem_dim)\n        nn.init.uniform_(embedding.weight, -0.1, 0.1)\n        return embedding\n\n    def _get_positional_encoding(self, position: int, d_model: int) -> np.ndarray:\n        pos_enc = np.zeros(d_model)\n        for i in range(0, d_model, 2):\n            pos_enc[i] = np.sin(position / (10000 ** (i / d_model)))\n            if i + 1 < d_model:\n                pos_enc[i+1] = np.cos(position / (10000 ** (i / d_model)))\n        return pos_enc\n\n    def _get_semantic_embeddings(self, sentence: str) -> np.ndarray:\n        semantic_data = []\n        words = sentence.split()\n\n        for word in words:\n            punct_idx = 0\n            for punct in self.punctuation.keys():\n                if word.endswith(punct):\n                    punct_idx = self.punct2indx[punct]\n                    break\n            emb_vec = self.sem_emb(torch.tensor([punct_idx], dtype=torch.long))\n            semantic_data.append(emb_vec.detach().numpy()[0])\n\n        return np.array(semantic_data)\n\n    def embed_sentence(self, sentence: str) -> torch.Tensor:\n        words = sentence.split()\n        if len(words) == 0:\n            return torch.empty(0, self.embed_dim)\n\n        semantic_embeddings = torch.tensor(\n            self._get_semantic_embeddings(sentence), dtype=torch.float32\n        )\n        # Fix the numpy array creation warning\n        pos_encodings = np.array([self._get_positional_encoding(idx, self.model_dim) for idx in range(len(words))])\n        positional_encodings = torch.tensor(pos_encodings, dtype=torch.float32)\n        combined_embeddings = torch.cat((semantic_embeddings, positional_encodings), dim=1)\n        return combined_embeddings \n\n    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, seq_len, _ = x.size()\n        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n    def self_attention(self, embeddings: torch.Tensor) -> torch.Tensor:\n        x = embeddings.unsqueeze(0)  \n        Q = self.W_q(x)\n        K = self.W_k(x)\n        V = self.W_v(x)\n        Q, K, V = map(self._split_heads, (Q, K, V)) \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attn_weights = torch.softmax(scores, dim=-1)\n        out = torch.matmul(attn_weights, V) \n        out = out.transpose(1, 2).contiguous().view(1, -1, self.embed_dim) \n        out = self.out_proj(out)\n        return out.squeeze(0) \n\n    def forward_with_attention(self, sentence: str) -> torch.Tensor:\n        embeddings = self.embed_sentence(sentence) \n        attended = self.self_attention(embeddings)\n        x = self.layer_norm(embeddings + attended)\n        ffn_out = self.ffn(x)\n        output = self.layer_norm(x + ffn_out)\n        return output\n\n\nclass SimpleDecoder(nn.Module):\n    def __init__(self, embed_dim, vocab_size, num_heads=2):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n        \n        \n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * 4),\n            nn.ReLU(),\n            nn.Linear(embed_dim * 4, embed_dim)\n        )\n        self.out_proj = nn.Linear(embed_dim, vocab_size)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.norm3 = nn.LayerNorm(embed_dim)\n    \n    def forward(self, tgt_tokens, memory):\n        if isinstance(tgt_tokens, torch.Tensor) and tgt_tokens.dtype == torch.long:\n            tgt_embeddings = self.token_embedding(tgt_tokens)\n        else:\n            tgt_embeddings = tgt_tokens\n            \n        if tgt_embeddings.dim() == 2:\n            tgt_embeddings = tgt_embeddings.unsqueeze(0)\n        if memory.dim() == 2:\n            memory = memory.unsqueeze(0)\n        \n        seq_len = tgt_embeddings.size(1)\n        causal_mask = self._causal_mask(seq_len)\n        \n        attn_out, _ = self.self_attn(tgt_embeddings, tgt_embeddings, tgt_embeddings, \n                                    attn_mask=causal_mask, is_causal=True)\n        x = self.norm1(tgt_embeddings + attn_out)\n        attn_out, _ = self.cross_attn(x, memory, memory)\n        x = self.norm2(x + attn_out)\n        \n        \n        ffn_out = self.ffn(x)\n        x = self.norm3(x + ffn_out)\n        \n        \n        logits = self.out_proj(x)\n        return logits.squeeze(0) if logits.size(0) == 1 else logits\n    \n    def _causal_mask(self, size):\n        mask = torch.triu(torch.ones(size, size), diagonal=1)\n        return mask.masked_fill(mask == 1, float('-inf'))\n\n\nclass TranslationTrainer:\n    def __init__(self, encoder, decoder, vocab_dict, learning_rate=0.001):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.vocab_dict = vocab_dict\n        self.reverse_vocab = {v: k for k, v in vocab_dict.items()}\n        \n        self.criterion = nn.CrossEntropyLoss(ignore_index=0) \n        \n        self.optimizer = optim.Adam(\n            list(encoder.parameters()) + list(decoder.parameters()), \n            lr=learning_rate,\n            weight_decay=1e-4\n        )\n        \n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)\n        \n        self.loss_history = []\n        self.bleu_history = []\n    \n    def tokens_to_ids(self, tokens: List[str]) -> List[int]:\n        \n        ids = []\n        for token in tokens:\n            if token in self.reverse_vocab:\n                ids.append(self.reverse_vocab[token])\n            else:\n                print(f\"Warning: Unknown token '{token}' replaced with <UNK>\")\n                ids.append(self.reverse_vocab['<UNK>'])\n        return ids\n    \n    def prepare_target_sequence(self, target_tokens: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n\n        target_with_bos = ['<BOS>'] + target_tokens\n        target_with_eos = target_tokens + ['<EOS>']\n        \n        input_ids = torch.tensor(self.tokens_to_ids(target_with_bos), dtype=torch.long)\n        target_ids = torch.tensor(self.tokens_to_ids(target_with_eos), dtype=torch.long)\n        \n        return input_ids, target_ids\n    \n    def train_step(self, source_sentence: str, target_tokens: List[str]) -> float:\n        self.encoder.train()\n        self.decoder.train()\n        self.optimizer.zero_grad()\n        \n        encoder_outputs = self.encoder.forward_with_attention(source_sentence)\n        \n    \n        decoder_input, target_labels = self.prepare_target_sequence(target_tokens)\n    \n        logits = self.decoder(decoder_input, encoder_outputs)\n        \n    \n        loss = self.criterion(logits.view(-1, logits.size(-1)), target_labels.view(-1))\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(\n            list(self.encoder.parameters()) + list(self.decoder.parameters()), \n            max_norm=1.0\n        )\n        \n        self.optimizer.step()\n        \n        return loss.item()\n    \n    def train_epoch(self, train_data: List[Dict], script_type='roman') -> float:\n        total_loss = 0.0\n        num_batches = 0\n    \n        shuffled_data = train_data.copy()\n        random.shuffle(shuffled_data)\n        \n        for sample in shuffled_data:\n            source = sample['source']\n            target_key = f'target_{script_type}'\n            \n            if target_key in sample:\n                target = sample[target_key]\n                loss = self.train_step(source, target)\n                total_loss += loss\n                num_batches += 1\n        \n        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n        self.loss_history.append(avg_loss)\n        \n        self.scheduler.step()\n        \n        return avg_loss\n    \n    def generate_translation(self, source_sentence: str, max_length: int = 15) -> List[str]:\n        \"\"\"Generate translation using greedy decoding\"\"\"\n        self.encoder.eval()\n        self.decoder.eval()\n        \n        with torch.no_grad():\n            encoder_outputs = self.encoder.forward_with_attention(source_sentence)\n            current_tokens = [self.reverse_vocab['<BOS>']]\n            generated_tokens = []\n            \n            for _ in range(max_length):\n                input_tensor = torch.tensor(current_tokens, dtype=torch.long)\n                logits = self.decoder(input_tensor, encoder_outputs)\n                next_token_id = torch.argmax(logits[-1]).item()\n                next_token = self.vocab_dict[next_token_id]\n                if next_token == '<EOS>':\n                    break\n                if next_token not in {'<BOS>', '<PAD>', '<UNK>'}:\n                    generated_tokens.append(next_token)\n                current_tokens.append(next_token_id)\n            \n            return generated_tokens\n\n\nclass EnglishHindiTranslationEvaluator:\n    def __init__(self, vocab_dict: Dict[int, str]):\n        self.vocab_dict = vocab_dict\n        self.reverse_vocab = {v: k for k, v in vocab_dict.items()}\n        self.special_tokens = {'<BOS>', '<EOS>', '<PAD>', '<UNK>'}\n    \n    def calculate_bleu_metrics(self, predicted_tokens: List[str], \n                              reference_tokens: List[str]) -> Dict[str, float]:\n        if not predicted_tokens or not reference_tokens:\n            return {'bleu': 0.0, 'bleu_1': 0.0, 'bleu_2': 0.0, 'bleu_4': 0.0}\n        \n        predicted_sentence = ' '.join(predicted_tokens)\n        reference_sentence = ' '.join(reference_tokens)\n        \n        bleu = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence])\n\n        try:\n            bleu_1 = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence], \n                                           max_ngram_order=1)\n            bleu_2 = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence], \n                                           max_ngram_order=2)\n            bleu_4 = sacrebleu.sentence_bleu(predicted_sentence, [reference_sentence], \n                                           max_ngram_order=4)\n        except:\n            bleu_1 = bleu_2 = bleu_4 = bleu\n        \n        return {\n            'bleu': bleu.score,\n            'bleu_1': bleu_1.score,\n            'bleu_2': bleu_2.score,\n            'bleu_4': bleu_4.score,\n            'brevity_penalty': bleu.bp,\n            'length_ratio': len(predicted_tokens) / len(reference_tokens) if reference_tokens else 0\n        }\n    \n    def evaluate_model(self, trainer, test_data: List[Dict], script_type='roman') -> Dict:\n        \"\"\"Evaluate model on test data\"\"\"\n        predictions = []\n        references = []\n        individual_scores = []\n        \n        target_key = f'target_{script_type}'\n        \n        for sample in test_data:\n            source = sample['source']\n            reference = sample[target_key]\n            prediction = trainer.generate_translation(source)\n            bleu_scores = self.calculate_bleu_metrics(prediction, reference)\n            individual_scores.append(bleu_scores)\n            \n            predictions.append(prediction)\n            references.append(reference)\n        \n        # Calculate average sentence BLEU\n        avg_bleu = np.mean([s['bleu'] for s in individual_scores])\n        \n        pred_sentences = [' '.join(tokens) for tokens in predictions]\n        ref_sentences = [' '.join(tokens) for tokens in references]\n        \n        if pred_sentences and ref_sentences:\n            corpus_bleu = sacrebleu.corpus_bleu(pred_sentences, [ref_sentences])\n        else:\n            corpus_bleu = sacrebleu.BLEU.compute_bleu([],[])\n        \n        avg_bleu_1 = np.mean([s['bleu_1'] for s in individual_scores])\n        avg_bleu_2 = np.mean([s['bleu_2'] for s in individual_scores])\n        avg_bleu_4 = np.mean([s['bleu_4'] for s in individual_scores])\n        \n        return {\n            'corpus_bleu': corpus_bleu.score,\n            'avg_sentence_bleu': avg_bleu,  # Fixed: this variable is now properly defined\n            'avg_bleu_1': avg_bleu_1,\n            'avg_bleu_2': avg_bleu_2,\n            'avg_bleu_4': avg_bleu_4,\n            'individual_scores': individual_scores,\n            'predictions': predictions,\n            'references': references\n        }\n\n\ndef create_english_hindi_vocab():\n    vocab_list = [\n    \n        '<PAD>', '<BOS>', '<EOS>', '<UNK>',\n        'i', 'want', 'to', 'eat', 'rice', 'and', 'dal', 'hello', 'how', 'are', 'you', \n        'good', 'morning', 'what', 'is', 'your', 'name', 'thank', 'very', 'much', \n        'the', 'book', 'water', 'please', 'where', 'go',\n    \n        'मैं', 'चावल', 'और', 'दाल', 'खाना', 'चाहता', 'हूं', 'नमस्ते', 'आप', 'कैसे',\n        'हैं', 'सुप्रभात', 'आपका', 'नाम', 'क्या', 'है', 'बहुत', 'धन्यवाद', 'पुस्तक', \n        'पानी', 'कृपया', 'कहां', 'जाना',\n    \n        'main', 'chawal', 'aur', 'daal', 'khaana', 'chahta', 'hun', 'namaste', 'aap', \n        'kaise', 'hain', 'suprabhat', 'aapka', 'naam', 'kya', 'hai', 'bahut', \n        'dhanyawad', 'pustak', 'paani', 'kripaya', 'kahan', 'jaana',\n        '.', ',', '?', '!'\n    ]\n    \n\n    vocab_dict = {i: token for i, token in enumerate(vocab_list)}\n    \n    print(f\"Created vocabulary with {len(vocab_dict)} tokens (IDs 0-{len(vocab_dict)-1})\")\n    return vocab_dict\n\n\ndef create_english_hindi_training_data():\n    training_data = [\n        {\n            'source': 'I want to eat rice and dal.',\n            'target_devanagari': ['मैं', 'चावल', 'और', 'दाल', 'खाना', 'चाहता', 'हूं'],\n            'target_roman': ['main', 'chawal', 'aur', 'daal', 'khaana', 'chahta', 'hun']\n        },\n        {\n            'source': 'Hello, how are you?',\n            'target_devanagari': ['नमस्ते', 'आप', 'कैसे', 'हैं'],\n            'target_roman': ['namaste', 'aap', 'kaise', 'hain']\n        },\n        {\n            'source': 'Good morning.',\n            'target_devanagari': ['सुप्रभात'],\n            'target_roman': ['suprabhat']\n        },\n        {\n            'source': 'What is your name?',\n            'target_devanagari': ['आपका', 'नाम', 'क्या', 'है'],\n            'target_roman': ['aapka', 'naam', 'kya', 'hai']\n        },\n        {\n            'source': 'Thank you very much.',\n            'target_devanagari': ['बहुत', 'धन्यवाद'],\n            'target_roman': ['bahut', 'dhanyawad']\n        },\n        {\n            'source': 'I want water please.',\n            'target_devanagari': ['मैं', 'पानी', 'चाहता', 'हूं', 'कृपया'],\n            'target_roman': ['main', 'paani', 'chahta', 'hun', 'kripaya']\n        },\n        {\n            'source': 'Where is the book?',\n            'target_devanagari': ['पुस्तक', 'कहां', 'है'],\n            'target_roman': ['pustak', 'kahan', 'hai']\n        },\n    ]\n    return training_data\n\n\ndef validate_training_data_vocab(training_data, vocab_dict):\n    \"\"\"Validate that all tokens in training data exist in vocabulary\"\"\"\n    reverse_vocab = {v: k for k, v in vocab_dict.items()}\n    missing_tokens = set()\n    \n    for sample in training_data:\n        for target_type in ['target_roman', 'target_devanagari']:\n            if target_type in sample:\n                for token in sample[target_type]:\n                    if token not in reverse_vocab:\n                        missing_tokens.add(token)\n    \n    if missing_tokens:\n        print(\"Missing tokens in vocabulary:\")\n        for token in sorted(missing_tokens):\n            print(f\"  '{token}'\")\n        return False\n    return True\n\n\ndef train_and_evaluate_model():\n    print(\"=== English-Hindi Translation Model Training ===\\n\")\n    vocab_dict = create_english_hindi_vocab()\n    training_data = create_english_hindi_training_data()\n    \n    print(f\"Vocabulary size: {len(vocab_dict)}\")\n    print(f\"Training samples: {len(training_data)}\")\n\n    if not validate_training_data_vocab(training_data, vocab_dict):\n        print(\"ERROR\")\n        return None, None, None\n    else:\n        print(\"training tokens found in vocabulary\")\n    \n    \n    encoder = SentenceEmbedderWithAttention(sem_dim=12, model_dim=12, num_heads=3)\n    decoder = SimpleDecoder(embed_dim=24, vocab_size=len(vocab_dict), num_heads=3)\n    \n    trainer = TranslationTrainer(encoder, decoder, vocab_dict, learning_rate=0.002)\n    evaluator = EnglishHindiTranslationEvaluator(vocab_dict)\n\n    num_epochs = 200\n    script_type = 'roman' \n    \n    print(f\"\\nTraining for {num_epochs} epochs on {script_type} script...\")\n    print(\"-\" * 60)\n    \n    try:\n        for epoch in range(num_epochs):\n            avg_loss = trainer.train_epoch(training_data, script_type)\n\n            if (epoch + 1) % 5 == 0 or epoch == 0:\n                results = evaluator.evaluate_model(trainer, training_data, script_type)\n                trainer.bleu_history.append(results['avg_sentence_bleu'])\n                \n                print(f\"Epoch {epoch+1:2d}: Loss = {avg_loss:.4f}, BLEU = {results['avg_sentence_bleu']:.2f}\")\n            else:\n                print(f\"Epoch {epoch+1:2d}: Loss = {avg_loss:.4f}\")\n                \n    except Exception as e:\n        print(f\"Training failed with error: {e}\")\n        return None, None, None\n    \n    print(\"\\n\" + \"--\"*60)\n    \n    print(\"\\n=== Translation Examples ===\")\n    test_sentences = [\n        \"Hello, how are you?\",\n        \"I want to eat rice and dal.\",\n        \"Good morning.\",\n        \"What is your name?\",\n        \"Thank you very much.\"\n    ]\n    \n    for sentence in test_sentences:\n        try:\n            prediction = trainer.generate_translation(sentence)\n            print(f\"english-- {sentence}\")\n            print(f\"hindi-- {' '.join(prediction)}\")\n            print(\"-\" * 40)\n        except Exception as e:\n            print(f\"Translation failed for '{sentence}': {e}\")\n    \n    try:\n        final_results = evaluator.evaluate_model(trainer, training_data, script_type)\n        \n        print(\"\\n=== Performance Metrics ===\")\n        print(f\"Corpus BLEU Score: {final_results['corpus_bleu']:.2f}\")\n        print(f\"Average Sentence BLEU: {final_results['avg_sentence_bleu']:.2f}\")\n        print(f\"Average BLEU-1: {final_results['avg_bleu_1']:.2f}\")\n        print(f\"Average BLEU-2: {final_results['avg_bleu_2']:.2f}\")\n        print(f\"Average BLEU-4: {final_results['avg_bleu_4']:.2f}\")\n        \n        print(f\"\\nLoss reduction: {trainer.loss_history[0]:.4f} → {trainer.loss_history[-1]:.4f}\")\n        if len(trainer.bleu_history) > 1:\n            print(f\"BLEU improvement: {trainer.bleu_history[0]:.2f} → {trainer.bleu_history[-1]:.2f}\")\n        \n        return trainer, evaluator, final_results\n        \n    except Exception as e:\n        print(f\"Final evaluation failed: {e}\")\n        return trainer, evaluator, None\n\nif __name__ == \"__main__\":\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    \n    trained_model, evaluator, results = train_and_evaluate_model()\n    \n    print(\"\\n=== Model Trained ===\")\n    print(\"You can now use trained_model.generate_translation() to translate new sentences!\")","metadata":{"_uuid":"05c85040-e263-466f-8c95-9087c8482679","_cell_guid":"9bacb56d-b293-4c0e-a362-2ed0d0217439","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-09-15T05:23:19.505923Z","iopub.execute_input":"2025-09-15T05:23:19.506215Z","iopub.status.idle":"2025-09-15T05:23:37.157220Z","shell.execute_reply.started":"2025-09-15T05:23:19.506180Z","shell.execute_reply":"2025-09-15T05:23:37.156552Z"}},"outputs":[{"name":"stdout","text":"=== English-Hindi Translation Model Training ===\n\nCreated vocabulary with 80 tokens (IDs 0-79)\nVocabulary size: 80\nTraining samples: 7\ntraining tokens found in vocabulary\n\nTraining for 200 epochs on roman script...\n------------------------------------------------------------\nEpoch  1: Loss = 4.6326, BLEU = 0.75\nEpoch  2: Loss = 3.9764\nEpoch  3: Loss = 3.5491\nEpoch  4: Loss = 3.2766\nEpoch  5: Loss = 3.0195, BLEU = 0.00\nEpoch  6: Loss = 2.7538\nEpoch  7: Loss = 2.5040\nEpoch  8: Loss = 2.2531\nEpoch  9: Loss = 2.0343\nEpoch 10: Loss = 1.8258, BLEU = 17.51\nEpoch 11: Loss = 1.6321\nEpoch 12: Loss = 1.4783\nEpoch 13: Loss = 1.3254\nEpoch 14: Loss = 1.2099\nEpoch 15: Loss = 1.0918, BLEU = 30.96\nEpoch 16: Loss = 0.9950\nEpoch 17: Loss = 0.9157\nEpoch 18: Loss = 0.8356\nEpoch 19: Loss = 0.7713\nEpoch 20: Loss = 0.7207, BLEU = 42.86\nEpoch 21: Loss = 0.6638\nEpoch 22: Loss = 0.6217\nEpoch 23: Loss = 0.5841\nEpoch 24: Loss = 0.5489\nEpoch 25: Loss = 0.5278, BLEU = 42.86\nEpoch 26: Loss = 0.4917\nEpoch 27: Loss = 0.4785\nEpoch 28: Loss = 0.4796\nEpoch 29: Loss = 0.4531\nEpoch 30: Loss = 0.4270, BLEU = 57.14\nEpoch 31: Loss = 0.4186\nEpoch 32: Loss = 0.4078\nEpoch 33: Loss = 0.3877\nEpoch 34: Loss = 0.3852\nEpoch 35: Loss = 0.3786, BLEU = 42.86\nEpoch 36: Loss = 0.3675\nEpoch 37: Loss = 0.3683\nEpoch 38: Loss = 0.3520\nEpoch 39: Loss = 0.3549\nEpoch 40: Loss = 0.3449, BLEU = 74.25\nEpoch 41: Loss = 0.3401\nEpoch 42: Loss = 0.3356\nEpoch 43: Loss = 0.3372\nEpoch 44: Loss = 0.3301\nEpoch 45: Loss = 0.3232, BLEU = 42.86\nEpoch 46: Loss = 0.3176\nEpoch 47: Loss = 0.3210\nEpoch 48: Loss = 0.3085\nEpoch 49: Loss = 0.3236\nEpoch 50: Loss = 0.2873, BLEU = 73.71\nEpoch 51: Loss = 0.2989\nEpoch 52: Loss = 0.2845\nEpoch 53: Loss = 0.2786\nEpoch 54: Loss = 0.2547\nEpoch 55: Loss = 0.2465, BLEU = 74.25\nEpoch 56: Loss = 0.2418\nEpoch 57: Loss = 0.2270\nEpoch 58: Loss = 0.2210\nEpoch 59: Loss = 0.2034\nEpoch 60: Loss = 0.1990, BLEU = 73.71\nEpoch 61: Loss = 0.2020\nEpoch 62: Loss = 0.1836\nEpoch 63: Loss = 0.2021\nEpoch 64: Loss = 0.2145\nEpoch 65: Loss = 0.1807, BLEU = 74.25\nEpoch 66: Loss = 0.1843\nEpoch 67: Loss = 0.1815\nEpoch 68: Loss = 0.1714\nEpoch 69: Loss = 0.1689\nEpoch 70: Loss = 0.1619, BLEU = 74.25\nEpoch 71: Loss = 0.1595\nEpoch 72: Loss = 0.1567\nEpoch 73: Loss = 0.1594\nEpoch 74: Loss = 0.1543\nEpoch 75: Loss = 0.1634, BLEU = 74.25\nEpoch 76: Loss = 0.1561\nEpoch 77: Loss = 0.1556\nEpoch 78: Loss = 0.1417\nEpoch 79: Loss = 0.1437\nEpoch 80: Loss = 0.1414, BLEU = 74.25\nEpoch 81: Loss = 0.1377\nEpoch 82: Loss = 0.1456\nEpoch 83: Loss = 0.1364\nEpoch 84: Loss = 0.1363\nEpoch 85: Loss = 0.1388, BLEU = 73.71\nEpoch 86: Loss = 0.1334\nEpoch 87: Loss = 0.1306\nEpoch 88: Loss = 0.1392\nEpoch 89: Loss = 0.1373\nEpoch 90: Loss = 0.1323, BLEU = 74.25\nEpoch 91: Loss = 0.1358\nEpoch 92: Loss = 0.1353\nEpoch 93: Loss = 0.1354\nEpoch 94: Loss = 0.1369\nEpoch 95: Loss = 0.1353, BLEU = 74.25\nEpoch 96: Loss = 0.1241\nEpoch 97: Loss = 0.1281\nEpoch 98: Loss = 0.1296\nEpoch 99: Loss = 0.1282\nEpoch 100: Loss = 0.1336, BLEU = 74.25\nEpoch 101: Loss = 0.1250\nEpoch 102: Loss = 0.1271\nEpoch 103: Loss = 0.1246\nEpoch 104: Loss = 0.1328\nEpoch 105: Loss = 0.1306, BLEU = 74.25\nEpoch 106: Loss = 0.1242\nEpoch 107: Loss = 0.1204\nEpoch 108: Loss = 0.1206\nEpoch 109: Loss = 0.1246\nEpoch 110: Loss = 0.1234, BLEU = 73.71\nEpoch 111: Loss = 0.1294\nEpoch 112: Loss = 0.1203\nEpoch 113: Loss = 0.1259\nEpoch 114: Loss = 0.1298\nEpoch 115: Loss = 0.1194, BLEU = 74.25\nEpoch 116: Loss = 0.1248\nEpoch 117: Loss = 0.1224\nEpoch 118: Loss = 0.1201\nEpoch 119: Loss = 0.1204\nEpoch 120: Loss = 0.1169, BLEU = 74.25\nEpoch 121: Loss = 0.1152\nEpoch 122: Loss = 0.1164\nEpoch 123: Loss = 0.1162\nEpoch 124: Loss = 0.1211\nEpoch 125: Loss = 0.1217, BLEU = 74.25\nEpoch 126: Loss = 0.1150\nEpoch 127: Loss = 0.1137\nEpoch 128: Loss = 0.1149\nEpoch 129: Loss = 0.1162\nEpoch 130: Loss = 0.1141, BLEU = 74.25\nEpoch 131: Loss = 0.1129\nEpoch 132: Loss = 0.1147\nEpoch 133: Loss = 0.1161\nEpoch 134: Loss = 0.1160\nEpoch 135: Loss = 0.1197, BLEU = 73.71\nEpoch 136: Loss = 0.1166\nEpoch 137: Loss = 0.1128\nEpoch 138: Loss = 0.1183\nEpoch 139: Loss = 0.1180\nEpoch 140: Loss = 0.1170, BLEU = 74.25\nEpoch 141: Loss = 0.1114\nEpoch 142: Loss = 0.1131\nEpoch 143: Loss = 0.1143\nEpoch 144: Loss = 0.1121\nEpoch 145: Loss = 0.1130, BLEU = 74.25\nEpoch 146: Loss = 0.1099\nEpoch 147: Loss = 0.1120\nEpoch 148: Loss = 0.1112\nEpoch 149: Loss = 0.1145\nEpoch 150: Loss = 0.1109, BLEU = 74.25\nEpoch 151: Loss = 0.1152\nEpoch 152: Loss = 0.1170\nEpoch 153: Loss = 0.1111\nEpoch 154: Loss = 0.1135\nEpoch 155: Loss = 0.1089, BLEU = 74.25\nEpoch 156: Loss = 0.1109\nEpoch 157: Loss = 0.1116\nEpoch 158: Loss = 0.1136\nEpoch 159: Loss = 0.1116\nEpoch 160: Loss = 0.1136, BLEU = 74.25\nEpoch 161: Loss = 0.1087\nEpoch 162: Loss = 0.1092\nEpoch 163: Loss = 0.1097\nEpoch 164: Loss = 0.1101\nEpoch 165: Loss = 0.1077, BLEU = 74.25\nEpoch 166: Loss = 0.1139\nEpoch 167: Loss = 0.1133\nEpoch 168: Loss = 0.1103\nEpoch 169: Loss = 0.1123\nEpoch 170: Loss = 0.1108, BLEU = 74.25\nEpoch 171: Loss = 0.1102\nEpoch 172: Loss = 0.1093\nEpoch 173: Loss = 0.1077\nEpoch 174: Loss = 0.1109\nEpoch 175: Loss = 0.1111, BLEU = 74.25\nEpoch 176: Loss = 0.1115\nEpoch 177: Loss = 0.1110\nEpoch 178: Loss = 0.1088\nEpoch 179: Loss = 0.1075\nEpoch 180: Loss = 0.1081, BLEU = 74.25\nEpoch 181: Loss = 0.1088\nEpoch 182: Loss = 0.1082\nEpoch 183: Loss = 0.1101\nEpoch 184: Loss = 0.1106\nEpoch 185: Loss = 0.1069, BLEU = 74.25\nEpoch 186: Loss = 0.1091\nEpoch 187: Loss = 0.1092\nEpoch 188: Loss = 0.1089\nEpoch 189: Loss = 0.1102\nEpoch 190: Loss = 0.1066, BLEU = 74.25\nEpoch 191: Loss = 0.1096\nEpoch 192: Loss = 0.1111\nEpoch 193: Loss = 0.1060\nEpoch 194: Loss = 0.1066\nEpoch 195: Loss = 0.1073, BLEU = 74.25\nEpoch 196: Loss = 0.1085\nEpoch 197: Loss = 0.1088\nEpoch 198: Loss = 0.1058\nEpoch 199: Loss = 0.1067\nEpoch 200: Loss = 0.1066, BLEU = 74.25\n\n------------------------------------------------------------------------------------------------------------------------\n\n=== Translation Examples ===\nenglish-- Hello, how are you?\nhindi-- namaste aap kaise hain\n----------------------------------------\nenglish-- I want to eat rice and dal.\nhindi-- main chawal aur daal khaana chahta hun\n----------------------------------------\nenglish-- Good morning.\nhindi-- suprabhat\n----------------------------------------\nenglish-- What is your name?\nhindi-- pustak kahan hai\n----------------------------------------\nenglish-- Thank you very much.\nhindi-- bahut dhanyawad\n----------------------------------------\n\n=== Performance Metrics ===\nCorpus BLEU Score: 72.82\nAverage Sentence BLEU: 74.25\nAverage BLEU-1: 74.25\nAverage BLEU-2: 74.25\nAverage BLEU-4: 74.25\n\nLoss reduction: 4.6326 → 0.1066\nBLEU improvement: 0.75 → 74.25\n\n=== Model Trained ===\nYou can now use trained_model.generate_translation() to translate new sentences!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"trained_model.generate_translation('Hello, how are you?')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:23:37.157927Z","iopub.execute_input":"2025-09-15T05:23:37.158231Z","iopub.status.idle":"2025-09-15T05:23:37.170774Z","shell.execute_reply.started":"2025-09-15T05:23:37.158206Z","shell.execute_reply":"2025-09-15T05:23:37.170186Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['namaste', 'aap', 'kaise', 'hain']"},"metadata":{}}],"execution_count":5}]}